---
title: "Principal Component Analysis with Python and sklearn"
data: 2020-11-01
tags: [PCA, Mario kart, data analyises]
header:
  image: "/images/chess.jpg"
excerpt: "Let‚Äôs play Mario Kart 64?"
---

# Introduction

Many of you have probably played Mario Kart 64, and each of you has a favorite character! I always played with Wario, which is certainly the best one üòä.
{: .text-justify}
In this game, we have 8 characters from the Nintendo franchise: Mario, Luigi, Princess Peach, Toad, Yoshi, Donkey Kong, Wario and Bowser. In general, we have the subdivision between light karts (Peach, Toad and Yoshi), medium karts (Mario and Luigi) and heavy karts (DK, Wario and Bowser), where it is common sense to say that the lighter ones have higher acceleration and the heavier ones have a higher speed; the medium ones are considered karts intermediaries.
{: .text-justify}
But, is it really that? Why Yoshi is from the light class and not from average class? Are Wario and Bowser the same? You probably never asked yourself these things before, but I asked myself and decided to investigate!
{: .text-justify}
To try to figure this out, we need to have some measurements for each kart. After a search on the internet, I found on [this site](https://gamefaqs.gamespot.com/n64/197860-mario-kart-64 / faqs / 27391) some features of each player. The features are:
{: .text-justify}
1. Time (s) required to reach 30 km/h;
2. Time (s) required to reach 50 km/h;
3. Maximum speed (km/h);
4. Time (s) required to reach maximum speed.

Unfortunately, I did not find information on the handling of each kart, which it would be important information for the analysis that we are going to do. The data is shown in Table 1.
{: .text-justify}

Table 1 ‚Äì Characteristics of each of the players in the Mario Kart 64 game.
{: .text-center}

| Player | TimeTo30kmph (s) |	TimeTo50kmph (s) |	TopSpeed (kmph) |	TimeToTopSpeed (kmph) |	WeightClass |
| :--- | :---: |	:---: |	:---: |	:---: |	:---: |
| Peach | 1.1 |	2.0 |	68 |	3.8 |	Light |
| Toad | 0.8 |	2.2 |	66 |	3.8 |	Light |
| Yoshi | 1.0 |	2.4 |	66 |	3.2 |	Light |
| Mario | 2.1 | 3.3	| 68	| 5.1	| Medium |
| Luigi	| 1.8	| 3.5	| 68	| 5.3	| Medium |
| Donkey Kong	| 2.0	| 2.6	| 70	| 4.2	| Heavy |
| Wario	| 2.0	| 2.7	| 70	| 4.0	| Heavy |
| Bowser	| 2.1	| 2.5	| 70	| 3.5	| Heavy |

When we look at the time to reach 30 km/h, we see a very clear difference between light and medium or heavy karts. But this characteristic is the same or is very confused between medium and heavy karts.
{: .text-justify}
As for the time to reach 50 km/h, we can see a difference between medium and heavy karts, where medium karts take longer to reach 50 km/h. Based on these data, we can say that light karts are the ones that accelerate faster at lower speeds.
When looking at the time needed to reach maximum speed, we see that the middle class is the slowest; however, the light and heavy classes overlapped. For example, look at Bowser that reaches maximum speed in less time than Toad and Peach, despite having a slower initial acceleration.
{: .text-justify}
Although it is possible to find some kind of pattern, as we saw through the maximum speed, in general, we don't have an obvious relationship between a kart being of the lightest class and accelerating faster and having a lower final speed, and it being heavier and then accelerating more slowly and have a higher final speed. In fact, we would even have something in that direction if it weren't for the middle class. But then, what characterizes a kart to be considered light, medium or heavy? Let's continue analyzing the data to try to find out.
{: .text-justify}
We could compare pairs of variables to try to find patterns more efficiently than just looking at numbers. An alternative is to several scatter plots (pair by pair) to try to find some kind of pattern, which can be seen in Figure 1.
{: .text-justify}

Figure 1 ‚Äì Scatter plot comparing each feature of the karts in Mario Kart 64.
{: .text-center}
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/scatter_plot.png" alt="cat" >

To try to find a pattern, we will need something more complex than an analysis of the numbers combined with a graphical analysis. I could use hierarchical group analysis, but I will use a statistical technique for reducing variables, which is principal component analysis (PCA). This technique consists of calculating new variables (called main components) based on the original variables.
{: .text-justify}
In fact, these main components are nothing more than linear combinations of the original variables. The great advantage of this method is that it has the potential to reduce the complexity of the data, making it easier to **identify patterns**, and maybe, the origin of the patterns. In other words, this technique has the potential to summarize all six graphs above in just 1 graph (depending on the data), which would certainly facilitate the interpretation of the results.
{: .text-justify}
We could use different programs/languages to do this, with R being the most obvious and simple choice (check [this incredible tutorial](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/)). We could also use STATISICA or even MATLAB, but they are very expensive programs, and you should never use a pirate program (really, since I developed one, I realized how easy it is to put some hidden lines of code to access data from users). For these and other reasons, I'm going to use Python, because the goal here is to use Python, because it's free and easy to use.
{: .text-justify}

# PCA in Python

Python provides at least two libraries to analyze the main components: sklearn and statsmodel. Each has its advantages and disadvantages. As far as I can tell, statsmodel is a little more versatile than sklearn. One of the advantages of statsmodel is that it allows you to use the NIPALS algorithm to perform the regression, which is a more usual method for experimental data than the SVG method, which is the only method available in sklearn. But sklearn seems to be used more in general, so I'm going to use sklearn.
{: .text-justify}
To do this study I am using Python 3.7, from the anaconda distribution and with the IDE Jupyter notebook. But you would not need to install anything at all to run this script on your computer (or cell phone). You could use [google colab](https://colab.research.google.com/), which is an online collaboration platform that allows you to use Python notebooks directly in your browser. Everything used here works on google colab without needing to install anything else. I will use Jupyter Notebook, as it is the way I am used to working with Python.
{: .text-justify}
With the Jupyter notebook open, we need to import some libraries that we will use when building the script:
{: .text-justify}
```python
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
%matplotlib inline
from platform import python_version
print(python_version())
```
Output: 3.7.3

Next, we need to import the data. I created an Excel file with the data, and generated a ‚Äú.csv‚Äù file to be able to import the data in a Data Frame format. You can find a link to download this spreadsheet at the end of this text. To import the file, you have to place it in the same folder where the Jupyter notebook file is (the same goes for google colab, where the file must be on the virtual drive where the colab notebook is). Hence:
{: .text-justify}

```python
df = pd.read_csv('mario_kart_64.csv', delimiter=';')
df
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/table_df.png" alt="cat" >

The ‚Äúdelimiter‚Äù parameter is necessary in this case due to the way I created the ‚Äú.csv‚Äù file, which was made from an Excel spreadsheet. My computer uses commas as the standard decimal separator, so the csv file has a comma separating the decimal places instead of a point as a csv (comma-separated value) file should be.
{: .text-justify}
To clarify the nomenclature I adopted, understand the players (Mario, Luigi, Peach, Toad, Yoshi, Donkey Kong, Wario and Bowser) as individuals, and the characteristics of the karts (TimeTo30kmph_ (s), TimeTo50kmph_ (s), TopSpeed_ (kmph), TimeToTopSpeed_ (kmph)) as variables.
{: .text-justify}
To work with PCA, we have to use only numeric variables. Then the "Character" and "WeightClass" columns need to be removed. But for the construction of some graphs, we will need these columns. So, it is easier to create a new Data Frame (which I will call df_scaled), but with only the numeric data:
{: .text-justify}

```python
df_scaled = df.drop(['Personagem', 'WeightClass'], axis=1)
df_scaled
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/table_df_scaled.png" alt="cat" >

The ‚Äú.drop‚Äù method requires two arguments: the first is a list with the name of the columns to be removed; the second is the axis that we want to remove (as we want the column to be removed, we use axis = 1). The new Data Frame (‚Äúdf_scaled‚Äù, which will be changed shortly) has only the numerical data.
{: .text-justify}
Something extremely important for PCA analysis is to verify the size of the data. For this, it is interesting to check the Boxplot of the original data.
{: .text-justify}

```python
plt.figure(figsize=(16,8))
df_scaled.boxplot()
plt.grid(False)
plt.show()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/boxplot.png" alt="cat" >

As we can see, with the exception of the maximum speed, the other variables have very close values. PCA analysis will always give higher importance to data with higher numerical value. So, if we use the data in its original form, the results will certainly be skewed to the maximum speed, since it has a numerical value much higher than the others. To check some basic statistics for the data, we can use the following method:
{: .text-justify}

```python
df_scaled.describe()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/pca_mk_64/describe.png" alt="cat" >

As we can see, the average of the TopSpeed variable is actually much higher than the others, which confirms a problem for the PCA.
{: .text-justify}
In addition, we have the problem that the units between the variables are not the same. We have time data (in seconds) that are related to the acceleration of each kart, and we have speed data (in km/h). This is also a serious problem for PCA analysis, as the PCA looks only at numbers, without considering units.
{: .text-justify}
For these reasons (and others not mentioned), it is common (and often advisable) to scale the data so that it starts to have an average equal to zero and a standard deviation equal to 1. Thus, all columns are in the same dimension, and the contribution for the main components it is similar. In this way, the data becomes dimensionless, and we no longer have problems with the different units. However, this is not always the right thing to do, it will depend on each case. For example, when we only have spectroscopic data, auto-scaling is probably not a good idea. In this case, it is essential.
{: .text-justify}
To transform the data for each variable with mean equal to zero and standard deviation equal to 1, we use Equation 1.
{: .text-justify}

\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s}
